{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bf0358b",
   "metadata": {
    "papermill": {
     "duration": 0.005721,
     "end_time": "2023-11-29T00:42:18.144352",
     "exception": false,
     "start_time": "2023-11-29T00:42:18.138631",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Mistral-7b + External Training Datasets (daigt)\n",
    "\n",
    "Credit : https://www.kaggle.com/code/minhsienweng/mistral-7b-v0-detection-train-infer\n",
    "\n",
    "Credit : https://www.kaggle.com/code/hotchpotch/infer-llm-detect-ai-comp-mistral-7b\n",
    "\n",
    "This notebook investigates the use of an pretrained LLM to identify texts generated by another LLM. We employ `Mistral-7b-v0` as an initial approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6dcb37",
   "metadata": {
    "papermill": {
     "duration": 0.004998,
     "end_time": "2023-11-29T00:42:18.154897",
     "exception": false,
     "start_time": "2023-11-29T00:42:18.149899",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Install library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae345f7b",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2023-11-29T00:42:18.167378Z",
     "iopub.status.busy": "2023-11-29T00:42:18.166423Z",
     "iopub.status.idle": "2023-11-29T00:43:29.562242Z",
     "shell.execute_reply": "2023-11-29T00:43:29.560951Z"
    },
    "papermill": {
     "duration": 71.410449,
     "end_time": "2023-11-29T00:43:29.570478",
     "exception": false,
     "start_time": "2023-11-29T00:42:18.160029",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install -q peft --no-index --find-links /kaggle/input/llm-detect-pip/peft-0.5.0-py3-none-any.whl\n",
    "# Eanble 4-bit CUDA functions for PyTorch\n",
    "!pip install -q bitsandbytes --no-index --find-link /kaggle/input/llm-detect-pip/bitsandbytes-0.41.1-py3-none-any.whl\n",
    "!pip install -q accelerate --no-index --find-links /kaggle/input/llm-detect-pip/accelerate-0.24.1-py3-none-any.whl \n",
    "!pip install -q transformers --no-index --find-links /kaggle/input/llm-detect-pip/transformers-4.34.1-py3-none-any.whl\n",
    "# Install language tool\n",
    "!!pip install -q language-tool-python --no-index --find-links /kaggle/input/daigt-misc/language_tool_python-2.7.1-py3-none-any.whl\n",
    "!!mkdir -p /root/.cache/language_tool_python/\n",
    "!!cp -r /kaggle/input/daigt-misc/lang57/LanguageTool-5.7 /root/.cache/language_tool_python/LanguageTool-5.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afda3eb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-29T00:43:29.586902Z",
     "iopub.status.busy": "2023-11-29T00:43:29.586510Z",
     "iopub.status.idle": "2023-11-29T00:43:51.232085Z",
     "shell.execute_reply": "2023-11-29T00:43:51.230826Z"
    },
    "papermill": {
     "duration": 21.655756,
     "end_time": "2023-11-29T00:43:51.235160",
     "exception": false,
     "start_time": "2023-11-29T00:43:29.579404",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.33.0\n",
      "0.5.0\n",
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import time, sys, gc, logging, random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType # type: ignore\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "from transformers import AutoTokenizer, LlamaForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorWithPadding\n",
    "import transformers\n",
    "import peft\n",
    "from accelerate import Accelerator\n",
    "import bitsandbytes\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from shutil import rmtree\n",
    "import language_tool_python\n",
    "import optuna\n",
    "import concurrent\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from concurrent.futures import wait\n",
    "\n",
    "print(transformers.__version__)\n",
    "print(peft.__version__)\n",
    "print(torch.__version__)\n",
    "\n",
    "language_tool = language_tool_python.LanguageTool('en-US')\n",
    "N_FOLD = 5\n",
    "SEED = 42\n",
    "DEBUG = True\n",
    "IS_TRAIN = False\n",
    "\n",
    "# Seed the same seed to all \n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "seed_everything()\n",
    "# Create new `pandas` methods which use `tqdm` progress\n",
    "# (can use tqdm_gui, optional kwargs, etc.)\n",
    "tqdm.pandas()\n",
    "\n",
    "log_level = \"DEBUG\"\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    "    level=logging.WARNING\n",
    ")\n",
    "\n",
    "# set the main code and the modules it uses to the same log-level according to the node\n",
    "transformers.utils.logging.set_verbosity(log_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24ef5e0",
   "metadata": {
    "papermill": {
     "duration": 0.008421,
     "end_time": "2023-11-29T00:43:51.251914",
     "exception": false,
     "start_time": "2023-11-29T00:43:51.243493",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84df3ac3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-29T00:43:51.270401Z",
     "iopub.status.busy": "2023-11-29T00:43:51.269371Z",
     "iopub.status.idle": "2023-11-29T00:43:51.288645Z",
     "shell.execute_reply": "2023-11-29T00:43:51.287800Z"
    },
    "papermill": {
     "duration": 0.030763,
     "end_time": "2023-11-29T00:43:51.290691",
     "exception": false,
     "start_time": "2023-11-29T00:43:51.259928",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cross validation\n",
    "def cv_split(train_data):\n",
    "    skf = StratifiedKFold(n_splits=N_FOLD, shuffle=True, random_state=SEED)\n",
    "    X = train_data.loc[:, train_data.columns != \"label\"]\n",
    "    y = train_data.loc[:, train_data.columns == \"label\"]\n",
    "\n",
    "    for fold, (train_index, valid_index) in enumerate(skf.split(X, y)):\n",
    "        train_data.loc[valid_index, \"fold\"] = fold\n",
    "\n",
    "    print(train_data.groupby(\"fold\")[\"label\"].value_counts())\n",
    "    display(train_data.head())\n",
    "    return train_data\n",
    "\n",
    "def pre_processing_text(text):\n",
    "    text = text.replace('\\n', ' ')\n",
    "    typos = language_tool.check(text) # typo is a list\n",
    "    # Check how many typos\n",
    "    #if len(typos) > 0:\n",
    "    #print(f\"The number of typos = {len(typos)}\\n {typos}\")\n",
    "    text = language_tool.correct(text)\n",
    "    return text\n",
    "\n",
    "# Run pre-processing texts in parallel\n",
    "def parallel_pre_processing_text(texts):\n",
    "    print(f\"Total number of texts {len(texts)}\")\n",
    "    results = []\n",
    "    # run 'pre_processing' fucntions in the process pool\n",
    "    with ThreadPoolExecutor(4) as executor:\n",
    "        # results = list(tqdm(executor.map(pre_processing_text, texts)))\n",
    "        # send in the tasks\n",
    "        futures = [executor.submit(pre_processing_text, text) for text in texts]\n",
    "        # wait for all tasks to complete\n",
    "        for future in futures:\n",
    "            results.append(future.result())\n",
    "            if len(results) % 100 == 0:\n",
    "                print(f\"Finished {len(results)} / {len(texts)}\\n\", end='', flush=True)\n",
    "    # wait for all tasks to complete\n",
    "    print(\"results\", len(results))\n",
    "    return results\n",
    "    \n",
    "    \n",
    "def load_train_data():\n",
    "    train_df = pd.read_csv(\"/kaggle/input/llm-detect-ai-generated-text/train_essays.csv\", sep=',')\n",
    "    train_prompts_df = pd.read_csv(\"/kaggle/input/llm-detect-ai-generated-text/train_prompts.csv\", sep=',')\n",
    "\n",
    "    # rename column generated to label and remove used 'id' and 'prompt_id' columns\n",
    "    # Label: 1 indicates generated texts (by LLMs) \n",
    "    train_df = train_df.rename(columns={'generated': 'label'})\n",
    "    train_df = train_df.reset_index(drop=True)\n",
    "    train_df = train_df.drop(['id', 'prompt_id'], axis=1)\n",
    "#     print(\"Start processing training data's text\")\n",
    "#     start = time.time()\n",
    "#     # Clear text in both train and test dataset\n",
    "#     train_df['text'] = train_df['text'].progress_apply(lambda text: pre_processing_text(text))\n",
    "#     display(train_df.head())\n",
    "#     print(f\"Correct the training data's texts with {time.time() - start : .1f} seconds\")\n",
    "    \n",
    "    # Include external data\n",
    "    external_df = pd.read_csv(\"/kaggle/input/daigt-v2-train-dataset/train_v2_drcat_02.csv\", sep=',')\n",
    "    # We only need 'text' and 'label' columns\n",
    "    external_df = external_df[[\"text\", \"label\"]]\n",
    "    external_df[\"label\"] = 1\n",
    "    \n",
    "#     print(\"Start processing external data's texts\")\n",
    "#     start = time.time()\n",
    "#     external_df['text'] = parallel_pre_processing_text(external_df['text'].to_list())\n",
    "#     print(f\"Correct the external data's texts with {time.time() - start : .1f} seconds\")\n",
    "#     # external_df['text'] = external_df['text'].map(lambda text: pre_processing_text(text))\n",
    "#     display(external_df.head())\n",
    "#     external_df.to_csv('train_v2_drcat_02_fixed.csv', index=False)\n",
    "    # Merge train and external data into train_data\n",
    "    train_data = pd.concat([train_df, external_df])\n",
    "    train_data.reset_index(inplace=True, drop=True)\n",
    "    # print(f\"Train data has shape: {train_data.shape}\")\n",
    "    print(f\"Train data {train_data.value_counts('label')}\") # 1: generated texts 0: human texts\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dc93647",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-29T00:43:51.304266Z",
     "iopub.status.busy": "2023-11-29T00:43:51.303462Z",
     "iopub.status.idle": "2023-11-29T00:43:53.497705Z",
     "shell.execute_reply": "2023-11-29T00:43:53.496811Z"
    },
    "papermill": {
     "duration": 2.203804,
     "end_time": "2023-11-29T00:43:53.500335",
     "exception": false,
     "start_time": "2023-11-29T00:43:51.296531",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data label\n",
      "1    44871\n",
      "0     1375\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cars. Cars have been around since they became ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Transportation is a large necessity in most co...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"America's love affair with it's vehicles seem...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How often do you ride in a car? Do you drive a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cars are a wonderful thing. They are perhaps o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46241</th>\n",
       "      <td>Dear Senator,\\n\\nI am writing to you today to ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46242</th>\n",
       "      <td>Dear Senator,\\n\\nI am writing to you today to ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46243</th>\n",
       "      <td>Dear Senator,\\n\\nI am writing to you today to ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46244</th>\n",
       "      <td>Dear Senator,\\n\\nI am writing to you today to ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46245</th>\n",
       "      <td>Dear Senator,\\n\\nI am writing to you today to ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46246 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "0      Cars. Cars have been around since they became ...      0\n",
       "1      Transportation is a large necessity in most co...      0\n",
       "2      \"America's love affair with it's vehicles seem...      0\n",
       "3      How often do you ride in a car? Do you drive a...      0\n",
       "4      Cars are a wonderful thing. They are perhaps o...      0\n",
       "...                                                  ...    ...\n",
       "46241  Dear Senator,\\n\\nI am writing to you today to ...      1\n",
       "46242  Dear Senator,\\n\\nI am writing to you today to ...      1\n",
       "46243  Dear Senator,\\n\\nI am writing to you today to ...      1\n",
       "46244  Dear Senator,\\n\\nI am writing to you today to ...      1\n",
       "46245  Dear Senator,\\n\\nI am writing to you today to ...      1\n",
       "\n",
       "[46246 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_train_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5de3cf",
   "metadata": {
    "papermill": {
     "duration": 0.006002,
     "end_time": "2023-11-29T00:43:53.513232",
     "exception": false,
     "start_time": "2023-11-29T00:43:53.507230",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load pretrained LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1428518",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-29T00:43:53.527305Z",
     "iopub.status.busy": "2023-11-29T00:43:53.527015Z",
     "iopub.status.idle": "2023-11-29T00:43:53.535586Z",
     "shell.execute_reply": "2023-11-29T00:43:53.534712Z"
    },
    "papermill": {
     "duration": 0.018332,
     "end_time": "2023-11-29T00:43:53.537780",
     "exception": false,
     "start_time": "2023-11-29T00:43:53.519448",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the pretrained model and add an extra layer with PEFT library for fine-tuning\n",
    "def load_model(fold):\n",
    "    TARGET_MODEL = \"/kaggle/input/mistral/pytorch/7b-v0.1-hf/1\"\n",
    "    # TARGET_MODEL = \"/kaggle/input/mistral-7b-v0-1/Mistral-7B-v0.1\"\n",
    "    # LoRA: Low-Rank Adaptation of Large Language Models\n",
    "    peft_config = LoraConfig(\n",
    "        r=64,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.SEQ_CLS,\n",
    "        inference_mode=False,\n",
    "        target_modules=[\n",
    "            \"q_proj\",\n",
    "            \"v_proj\"\n",
    "        ],\n",
    "    )\n",
    "    # Enable GPU to run the model with 4bit\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "    # Load the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(TARGET_MODEL, use_fast=False)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    # Load the model\n",
    "    base_model = LlamaForSequenceClassification.from_pretrained(TARGET_MODEL,\n",
    "                                                                num_labels=2, # label is 0 or 1\n",
    "                                                                quantization_config=bnb_config,                                                                 \n",
    "                                                                device_map=\"auto\")\n",
    "    base_model.config.pretraining_tp = 1 # 1 is 7b\n",
    "    base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    \n",
    "    if IS_TRAIN:\n",
    "        # Parameter-Efficient Fine-Tuning (PEFT) methods enable efficient adaptation of \n",
    "        # pre-trained language models (PLMs) to various downstream applications \n",
    "        # without fine-tuning all the model's parameters. \n",
    "        # https://github.com/huggingface/peft\n",
    "        model = get_peft_model(base_model, peft_config)\n",
    "    else:\n",
    "        OUTPUT_DIR = f\"/kaggle/input/mistral-7b-v0-for-llm-detecting-competition/mistral_7b_fold{fold}\"\n",
    "        # OUTPUT_DIR = f\"/kaggle/working/mistral_7b_fold{fold}\"\n",
    "        # Load the pretrained model with PEFT\n",
    "        model = PeftModel.from_pretrained(base_model, str(OUTPUT_DIR))\n",
    "    \n",
    "    model.print_trainable_parameters() # Display the trainable parameters\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5df06a4",
   "metadata": {
    "papermill": {
     "duration": 0.005868,
     "end_time": "2023-11-29T00:43:53.549967",
     "exception": false,
     "start_time": "2023-11-29T00:43:53.544099",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train the LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7870196f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-29T00:43:53.563001Z",
     "iopub.status.busy": "2023-11-29T00:43:53.562690Z",
     "iopub.status.idle": "2023-11-29T00:43:53.579190Z",
     "shell.execute_reply": "2023-11-29T00:43:53.578509Z"
    },
    "papermill": {
     "duration": 0.025258,
     "end_time": "2023-11-29T00:43:53.581101",
     "exception": false,
     "start_time": "2023-11-29T00:43:53.555843",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_function(examples, tokenizer, max_length=512):\n",
    "    examples[\"text\"] = list(map(lambda text: pre_processing_text(text), examples[\"text\"]))\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=max_length, padding=True)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    accuracy_val = accuracy_score(labels, predictions)\n",
    "    roc_auc_val = roc_auc_score(labels, predictions)\n",
    "    r = { \"accuracy\": accuracy_val,\n",
    "          \"roc_auc\": roc_auc_val}\n",
    "    # logging.debug(f'{r}')\n",
    "    return r\n",
    "\n",
    "\n",
    "def train_model_by_fold(fold):\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(f\"Start training the fold {fold} model\")\n",
    "    # Create train and valid dataset for a fold\n",
    "    fold_valid_df = train_data[train_data[\"fold\"] == fold]\n",
    "    fold_train_df = train_data[train_data[\"fold\"] != fold]\n",
    "    # Train the model with small (for debugging) or large samples\n",
    "    if DEBUG:\n",
    "        fold_train_df = fold_train_df.sample(frac =.05, random_state=SEED)\n",
    "        fold_valid_df = fold_valid_df.sample(frac =.05, random_state=SEED)\n",
    "    else:\n",
    "        fold_train_df = fold_train_df.sample(frac =.3, random_state=SEED)\n",
    "        fold_valid_df = fold_valid_df.sample(frac =.3, random_state=SEED)\n",
    "\n",
    "    print(f'fold_train_df {fold_train_df.groupby(\"fold\")[\"label\"].value_counts()}')\n",
    "    print(f'fold_valid_df {fold_valid_df.groupby(\"fold\")[\"label\"].value_counts()}')\n",
    "    # create the dataset\n",
    "    train_ds = Dataset.from_pandas(fold_train_df)\n",
    "    valid_ds = Dataset.from_pandas(fold_valid_df)\n",
    "\n",
    "    # Load the pretrained model and tokenizer\n",
    "    model, tokenizer = load_model(fold)\n",
    "\n",
    "    # Tokenize the train and valid dataset and pass tokenizer as function argument\n",
    "    train_tokenized_ds = train_ds.map(preprocess_function, batched=True,\n",
    "                                      fn_kwargs={\"tokenizer\": tokenizer})\n",
    "    valid_tokenized_ds = valid_ds.map(preprocess_function, batched=True,\n",
    "                                      fn_kwargs={\"tokenizer\": tokenizer})\n",
    "    # Create data collator with padding (padding to the longest sequence)\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=\"longest\")\n",
    "\n",
    "    # Start training processing        \n",
    "    TMP_DIR = Path(f\"/kaggle/tmp/mistral_7b_fold{fold}/\")\n",
    "    TMP_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    STEPS = 5 if DEBUG else 20\n",
    "    EPOCHS = 1 if DEBUG else 10\n",
    "    BATCH_SIZE = 2\n",
    "    training_args = TrainingArguments(output_dir=TMP_DIR,\n",
    "                                      learning_rate=5e-5,\n",
    "                                      per_device_train_batch_size=BATCH_SIZE,\n",
    "                                      per_device_eval_batch_size=1,\n",
    "                                      gradient_accumulation_steps=16,\n",
    "                                      max_grad_norm=0.3,\n",
    "                                      optim='paged_adamw_32bit',\n",
    "                                      lr_scheduler_type=\"cosine\",\n",
    "                                      num_train_epochs=EPOCHS,\n",
    "                                      weight_decay=0.01,\n",
    "                                      evaluation_strategy=\"epoch\",\n",
    "                                      save_strategy=\"epoch\",\n",
    "                                      load_best_model_at_end=True,\n",
    "                                      push_to_hub=False,\n",
    "                                      warmup_steps=STEPS,\n",
    "                                      eval_steps=STEPS,\n",
    "                                      logging_steps=STEPS,\n",
    "                                      report_to='none', # if DEBUG else 'wandb'\n",
    "                                      log_level='warning', # 'warning' is default level \n",
    "                                     )\n",
    "\n",
    "\n",
    "    # Create the trainer \n",
    "    trainer = Trainer(model=model,\n",
    "                      args=training_args,\n",
    "                      train_dataset=train_tokenized_ds,\n",
    "                      eval_dataset=valid_tokenized_ds,\n",
    "                      tokenizer=tokenizer,\n",
    "                      data_collator=data_collator,\n",
    "                      compute_metrics=compute_metrics)\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    OUTPUT_DIR = Path(f\"/kaggle/working/mistral_7b_fold{fold}/\")\n",
    "    OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "    # Save the fine-tuned model\n",
    "    trainer.save_model(output_dir=str(OUTPUT_DIR))\n",
    "    print(f\"=== Finish the training for fold {fold} ===\")\n",
    "    del model, trainer, tokenizer\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f438c7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-29T00:43:53.593640Z",
     "iopub.status.busy": "2023-11-29T00:43:53.593373Z",
     "iopub.status.idle": "2023-11-29T00:43:53.598389Z",
     "shell.execute_reply": "2023-11-29T00:43:53.597558Z"
    },
    "papermill": {
     "duration": 0.013347,
     "end_time": "2023-11-29T00:43:53.600218",
     "exception": false,
     "start_time": "2023-11-29T00:43:53.586871",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check if we need to fine-tune the LLM model\n",
    "if IS_TRAIN:\n",
    "    start = time.time()\n",
    "    # Load train data\n",
    "    train_data = load_train_data()\n",
    "    # Cross validation with 5 fold\n",
    "    train_data = cv_split(train_data)\n",
    "    # Train the model  \n",
    "    fold = 0\n",
    "    train_model_by_fold(0)\n",
    "    #     # Add multiple threads to run each fold model concurrently\n",
    "\n",
    "    #with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n",
    "   #     futures = [executor.submit(train_model_by_fold, fold) for fold in range(2)]\n",
    "   #     # wait for all tasks to complete\n",
    "    #    wait(futures)\n",
    "    #    print('All training tasks are done!')\n",
    "    \n",
    "    #for idx, fold in enumerate(range(N_FOLD)):\n",
    "    sys.exit(f\"Training time of fold {fold} = {time.time() - start: .1f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1291a2ef",
   "metadata": {
    "papermill": {
     "duration": 0.005546,
     "end_time": "2023-11-29T00:43:53.611460",
     "exception": false,
     "start_time": "2023-11-29T00:43:53.605914",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Infer the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02d81766",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-29T00:43:53.624145Z",
     "iopub.status.busy": "2023-11-29T00:43:53.623892Z",
     "iopub.status.idle": "2023-11-29T00:44:00.215794Z",
     "shell.execute_reply": "2023-11-29T00:44:00.214652Z"
    },
    "papermill": {
     "duration": 6.601307,
     "end_time": "2023-11-29T00:44:00.218426",
     "exception": false,
     "start_time": "2023-11-29T00:43:53.617119",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:06<00:00,  2.19s/it]\n"
     ]
    }
   ],
   "source": [
    "import concurrent\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from concurrent.futures import wait\n",
    "from scipy.special import expit as sigmoid\n",
    "# Load test data\n",
    "test_df = pd.read_csv(\"/kaggle/input/llm-detect-ai-generated-text/test_essays.csv\", sep=',')\n",
    "test_df = test_df.rename(columns={'generated': 'label'})\n",
    "test_df['text'] = test_df['text'].progress_apply(lambda text: pre_processing_text(text))\n",
    "# print(f'test_df.shape: {test_df.shape}')\n",
    "# test_df.head(3)\n",
    "\n",
    "def clear_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# Sigmoid activation function can map 'x' between 0 and 1\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x)) \n",
    "\n",
    "def predict_result_by_fold(fold):\n",
    "    clear_memory()\n",
    "    print(f\"=== Start prediction with {fold} ===\")\n",
    "    model, tokenizer = load_model(fold) \n",
    "    # Load the test dataframe as dataset\n",
    "    test_ds = Dataset.from_pandas(test_df)\n",
    "    test_tokenized_ds = test_ds.map(preprocess_function, batched=True,\n",
    "                                    fn_kwargs={\"tokenizer\": tokenizer})\n",
    "    # Data collator\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer, \n",
    "                                            padding=\"longest\")\n",
    "    # Create the trainer\n",
    "    trainer = Trainer(model=model,\n",
    "                      tokenizer=tokenizer,\n",
    "                      data_collator=data_collator)\n",
    "    pred_output = trainer.predict(test_tokenized_ds)\n",
    "    logits = pred_output.predictions\n",
    "    # Apply sigmoid to \n",
    "    probs = sigmoid(logits[:, 1])\n",
    "    print(f\"fold = {fold} probs = {probs}\")\n",
    "    global predictions\n",
    "    for i, prob in enumerate(probs):\n",
    "        predictions[i].append(prob)  \n",
    "    # Clear memory\n",
    "    del model, trainer, tokenizer, test_ds, test_tokenized_ds, data_collator\n",
    "    clear_memory()\n",
    "    \n",
    "def predict_result():\n",
    "    global predictions\n",
    "    predictions = [[] for i in range(len(test_df))]\n",
    "    start = time.time()\n",
    "    print(f\"=== Begin prediction  ===\")\n",
    "\n",
    "    #for fold in range(N_FOLD):\n",
    "    #    predict_result_by_fold(fold)\n",
    "    fold = 0\n",
    "    predict_result_by_fold(fold)\n",
    "    print(f\"Finish prediction in {time.time() - start: .1f} seconds\")\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37ea33f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-29T00:44:00.246188Z",
     "iopub.status.busy": "2023-11-29T00:44:00.245762Z",
     "iopub.status.idle": "2023-11-29T00:46:47.505399Z",
     "shell.execute_reply": "2023-11-29T00:46:47.504283Z"
    },
    "papermill": {
     "duration": 167.274924,
     "end_time": "2023-11-29T00:46:47.507489",
     "exception": false,
     "start_time": "2023-11-29T00:44:00.232565",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file tokenizer.model\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.\n",
      "loading configuration file /kaggle/input/mistral/pytorch/7b-v0.1-hf/1/config.json\n",
      "You are using a model of type mistral to instantiate a model of type llama. This is not supported for all configurations of models and can yield errors.\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.33.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file /kaggle/input/mistral/pytorch/7b-v0.1-hf/1/pytorch_model.bin.index.json\n",
      "Instantiating LlamaForSequenceClassification model under default dtype torch.float16.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Begin prediction  ===\n",
      "=== Start prediction with 0 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected 4-bit loading: activating 4-bit loading for this model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "163e6fa1699d41c4bbed7745a113c71a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /kaggle/input/mistral/pytorch/7b-v0.1-hf/1 were not used when initializing LlamaForSequenceClassification: ['lm_head.weight']\n",
      "- This IS expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/mistral/pytorch/7b-v0.1-hf/1 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 16,384 || all params: 7,114,084,352 || trainable%: 0.00023030370725635164\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86f1da9a10c9486095af0e0fea181f5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "You have loaded a model on multiple GPUs. `is_model_parallel` attribute will be force-set to `True` to avoid any unexpected behavior such as device placement mismatching.\n",
      "The model is quantized. To train this model you need to add additional modules inside the model such as adapters using `peft` library and freeze the model weights. Please check the examples in https://github.com/huggingface/peft for more details.\n",
      "The following columns in the test set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: prompt_id, text, id. If prompt_id, text, id are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 3\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold = 0 probs = [0.999 0.998 0.999]\n",
      "Finish prediction in  167.3 seconds\n",
      "[0.999, 0.998, 0.999]\n"
     ]
    }
   ],
   "source": [
    "predictions = predict_result()\n",
    "probs = [np.mean(pred) for pred in predictions] \n",
    "print(probs)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48cc47fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-29T00:46:47.526240Z",
     "iopub.status.busy": "2023-11-29T00:46:47.525926Z",
     "iopub.status.idle": "2023-11-29T00:46:47.541389Z",
     "shell.execute_reply": "2023-11-29T00:46:47.540479Z"
    },
    "papermill": {
     "duration": 0.027223,
     "end_time": "2023-11-29T00:46:47.543541",
     "exception": false,
     "start_time": "2023-11-29T00:46:47.516318",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000aaaa</td>\n",
       "      <td>0.999023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1111bbbb</td>\n",
       "      <td>0.998047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2222cccc</td>\n",
       "      <td>0.999023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  generated\n",
       "0  0000aaaa   0.999023\n",
       "1  1111bbbb   0.998047\n",
       "2  2222cccc   0.999023"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub['id'] = test_df['id']\n",
    "sub['generated'] = probs\n",
    "sub.to_csv('submission.csv', index=False)\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f50981c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-29T00:46:47.565568Z",
     "iopub.status.busy": "2023-11-29T00:46:47.565008Z",
     "iopub.status.idle": "2023-11-29T00:46:47.569311Z",
     "shell.execute_reply": "2023-11-29T00:46:47.568326Z"
    },
    "papermill": {
     "duration": 0.017664,
     "end_time": "2023-11-29T00:46:47.571430",
     "exception": false,
     "start_time": "2023-11-29T00:46:47.553766",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !ls -alh /kaggle/working/\n",
    "# !zip -r result.zip /kaggle/working\n",
    "# from IPython.display import FileLink\n",
    "# FileLink(r'result.zip')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 6888007,
     "sourceId": 61542,
     "sourceType": "competition"
    },
    {
     "datasetId": 3939470,
     "sourceId": 6853624,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3945154,
     "sourceId": 6865136,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4005256,
     "sourceId": 6977472,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4049286,
     "sourceId": 7070322,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 148861315,
     "sourceType": "kernelVersion"
    },
    {
     "modelInstanceId": 3899,
     "sourceId": 5111,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30559,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 276.234523,
   "end_time": "2023-11-29T00:46:50.605176",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-11-29T00:42:14.370653",
   "version": "2.4.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0618cb24da534cb2864eea072efa40dc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2d0cb8802e834bfca6e4c479b968a0a2",
       "placeholder": "​",
       "style": "IPY_MODEL_f23b32715faa4ac1a6cbab9f30b1aba7",
       "value": " 1/1 [00:00&lt;00:00,  2.72ba/s]"
      }
     },
     "163e6fa1699d41c4bbed7745a113c71a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_4ff0c818a5224834b857ae68dc067c15",
        "IPY_MODEL_22dcebc235a043ec8c1a189eb9d486f2",
        "IPY_MODEL_e06b4b4a0e7548d48d63a605f6487b9c"
       ],
       "layout": "IPY_MODEL_d754270d5d8847a489108d0388dc720c"
      }
     },
     "1eaf459bcabe420385e628fc2612b4b0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_24def21af8964e9e8a03edd38cf1fb2d",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_94dc18f7970d434d87793510f1743989",
       "value": 1.0
      }
     },
     "22dcebc235a043ec8c1a189eb9d486f2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_97f25c040d2b486094c5bb93c686de2d",
       "max": 2.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_c8e132837dea4c46abca75cd661da461",
       "value": 2.0
      }
     },
     "24def21af8964e9e8a03edd38cf1fb2d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2d0cb8802e834bfca6e4c479b968a0a2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3ce42b8cb4574a7e9683113330e50605": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "4ff0c818a5224834b857ae68dc067c15": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_9ae0a37019c541c68f25f7996a8ed08f",
       "placeholder": "​",
       "style": "IPY_MODEL_620e7177dec448bdad897af554b63a47",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "620e7177dec448bdad897af554b63a47": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "86f1da9a10c9486095af0e0fea181f5e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_cb010ba388284aeb9766c192a4d6b560",
        "IPY_MODEL_1eaf459bcabe420385e628fc2612b4b0",
        "IPY_MODEL_0618cb24da534cb2864eea072efa40dc"
       ],
       "layout": "IPY_MODEL_87b070a6d06a496f99e80f2e5012e107"
      }
     },
     "87b070a6d06a496f99e80f2e5012e107": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "94dc18f7970d434d87793510f1743989": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "97f25c040d2b486094c5bb93c686de2d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9ae0a37019c541c68f25f7996a8ed08f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b63af5ed0637494eb89daf6ca155dced": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c8e132837dea4c46abca75cd661da461": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "cb010ba388284aeb9766c192a4d6b560": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ef71b7ca175e45a18da155985ad7b867",
       "placeholder": "​",
       "style": "IPY_MODEL_3ce42b8cb4574a7e9683113330e50605",
       "value": "100%"
      }
     },
     "d754270d5d8847a489108d0388dc720c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e06b4b4a0e7548d48d63a605f6487b9c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b63af5ed0637494eb89daf6ca155dced",
       "placeholder": "​",
       "style": "IPY_MODEL_f081faed390642f9a7ae75385f7dbf6e",
       "value": " 2/2 [02:26&lt;00:00, 68.90s/it]"
      }
     },
     "ef71b7ca175e45a18da155985ad7b867": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f081faed390642f9a7ae75385f7dbf6e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "f23b32715faa4ac1a6cbab9f30b1aba7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
